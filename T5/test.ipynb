{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-25 11:59:37.777534: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 11:59:37.789226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742903977.803695  888827 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742903977.808039  888827 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 11:59:37.823040: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:03<00:00, 4009.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 4008.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 2874.16 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02aa490fc410:888827:888827 [0] NCCL INFO cudaDriverVersion 12020\n",
      "02aa490fc410:888827:888827 [0] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>\n",
      "02aa490fc410:888827:888827 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\n",
      "02aa490fc410:888827:888827 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\n",
      "02aa490fc410:888827:888827 [0] NCCL INFO NET/Plugin: Using internal network plugin.\n",
      "NCCL version 2.21.5+cuda12.4\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Using non-device net plugin version 0\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Using network Socket\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Using non-device net plugin version 0\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Using network Socket\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Using non-device net plugin version 0\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Using network Socket\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Using non-device net plugin version 0\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Using network Socket\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO ncclCommInitRank comm 0x175c0bc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 51000 commId 0xc722925f3359c252 - Init START\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO ncclCommInitRank comm 0x175cbe60 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c3000 commId 0xc722925f3359c252 - Init START\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO ncclCommInitRank comm 0x175c6510 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 8a000 commId 0xc722925f3359c252 - Init START\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO ncclCommInitRank comm 0x175bb270 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 18000 commId 0xc722925f3359c252 - Init START\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Setting affinity for GPU 1 to 0fff,ffffffff,ffffffff\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO NVLS multicast support is not available on dev 1\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Setting affinity for GPU 0 to 0fff,ffffffff,ffffffff\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO NVLS multicast support is not available on dev 0\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Setting affinity for GPU 2 to 0fff,ffffffff,ffffffff\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO NVLS multicast support is not available on dev 2\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Setting affinity for GPU 3 to 0fff,ffffffff,ffffffff\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO NVLS multicast support is not available on dev 3\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO comm 0x175c0bc0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO comm 0x175cbe60 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO comm 0x175bb270 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO P2P Chunksize set to 131072\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO comm 0x175c6510 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Channel 00/02 :    0   1   2   3\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Channel 01/02 :    0   1   2   3\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO P2P Chunksize set to 131072\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Connected all rings\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Connected all rings\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Connected all rings\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Connected all rings\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO Connected all trees\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO Connected all trees\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO Connected all trees\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO Connected all trees\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n",
      "02aa490fc410:888827:889210 [3] NCCL INFO ncclCommInitRank comm 0x175cbe60 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c3000 commId 0xc722925f3359c252 - Init COMPLETE\n",
      "02aa490fc410:888827:889208 [1] NCCL INFO ncclCommInitRank comm 0x175c0bc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 51000 commId 0xc722925f3359c252 - Init COMPLETE\n",
      "02aa490fc410:888827:889209 [2] NCCL INFO ncclCommInitRank comm 0x175c6510 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 8a000 commId 0xc722925f3359c252 - Init COMPLETE\n",
      "02aa490fc410:888827:889207 [0] NCCL INFO ncclCommInitRank comm 0x175bb270 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 18000 commId 0xc722925f3359c252 - Init COMPLETE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='6300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  86/6300 00:55 < 1:08:48, 1.51 it/s, Epoch 1.35/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.056484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m\n\u001b[1;32m     71\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     72\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     73\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     74\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     75\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 6. Train and Save\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Dataset and Tokenizer\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Preprocess Data for T5\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"emotion classification: \" + text for text in examples[\"text\"]]\n",
    "    labels = [[\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\",\"disgust\"][label] for label in examples[\"label\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            labels,\n",
    "            max_length=8,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 3. Load T5-base Model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 4. Training Configuration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-emotion\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# 5. Create Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# 6. Train and Save\n",
    "trainer.train()\n",
    "model.save_pretrained(\"models/\")\n",
    "tokenizer.save_pretrained(\"models/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score: 0.0860\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Tokenize test dataset\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)  # Get predicted labels\n",
    "labels = test_dataset[\"label\"]  # True labels\n",
    "\n",
    "# Calculate F1 Macro score\n",
    "f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "print(f\"F1 Macro Score: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at models/ and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'joy', 'sadness', 'joy', 'joy', 'sadness', 'sadness', 'joy', 'joy', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness']\n",
      "F1 Score: 0.1741\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "model_path = \"models/\"  # Path where the model was saved\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# New data with simple sentences\n",
    "new_data = [\n",
    "    {\"text\": \"Iâ€™m scared my credit score will never recover.\", \"label\": \"fear\"},\n",
    "    {\"text\": \"Itâ€™s unfair how high the debt settlement fees are!\", \"label\": \"anger\"},\n",
    "    {\"text\": \"My low credit score makes me feel hopeless.\", \"label\": \"sadness\"},\n",
    "    {\"text\": \"Iâ€™m so happy my credit score is improving!\", \"label\": \"joy\"},\n",
    "    {\"text\": \"What if my credit score drops even more?\", \"label\": \"fear\"},\n",
    "    {\"text\": \"Why does debt settlement take so long?\", \"label\": \"anger\"},\n",
    "    {\"text\": \"I feel defeated seeing my credit card debt.\", \"label\": \"sadness\"},\n",
    "    {\"text\": \"Itâ€™s great that I finally paid off my debt!\", \"label\": \"joy\"},\n",
    "    {\"text\": \"Lenders charging extra interest disgusts me.\", \"label\": \"anger\"},\n",
    "    {\"text\": \"Iâ€™m terrified of my debt going to collections.\", \"label\": \"fear\"},\n",
    "    {\"text\": \"Credit card fees are way too high!\", \"label\": \"anger\"},\n",
    "    {\"text\": \"I feel lost trying to fix my bad credit.\", \"label\": \"sadness\"},\n",
    "    {\"text\": \"Iâ€™m relieved my loan got approved!\", \"label\": \"joy\"},\n",
    "    {\"text\": \"I hate that my interest rate keeps rising.\", \"label\": \"disgust\"},\n",
    "    {\"text\": \"I am scared and What if my loan application gets rejected?\", \"label\": \"fear\"},\n",
    "    {\"text\": \"Why does fixing credit take so long?\", \"label\": \"anger\"}\n",
    "]\n",
    "\n",
    "\n",
    "# Split new data into texts and labels\n",
    "texts = [item[\"text\"] for item in new_data]\n",
    "labels = [item[\"label\"] for item in new_data]\n",
    "\n",
    "# Tokenize unseen data\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}  # Move inputs to correct device\n",
    "\n",
    "# Perform inference\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)  # Get label with highest probability\n",
    "\n",
    "# Map predictions to label names\n",
    "label_map = {i: label for i, label in enumerate(dataset[\"train\"].features[\"label\"].names)}  # Assuming label_map is defined\n",
    "predicted_labels = [label_map[pred.item()] for pred in predictions]\n",
    "\n",
    "print(predicted_labels)\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(labels, predicted_labels, average=\"weighted\")  # Use \"weighted\" for imbalanced classes\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
